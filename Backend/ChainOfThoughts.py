from operator import itemgetter
import openai
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import JSONLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.output_parsers import JsonOutputParser
from dotenv import load_dotenv
from LoadandEmbed import create_db


# Loaded all the secret keys
load_dotenv()

# created openai instance to interact with the openai models
LLM = ChatOpenAI(model="gpt-4o-mini")

string_parser = StrOutputParser()
json_parser = JsonOutputParser()

# Loaded all the product details from the json file
docs = JSONLoader(file_path="Backend/products.json",
                  jq_schema=".", text_content=False)
loaded_docs = docs.load()


# cleaned the data and converted it to string so that it can be converted to embeddings and store it in a vector store
for doc in loaded_docs:
    doc.page_content = str(doc.page_content)


# Function that provides answer to queries related to general product information
def chian_of_thoughts_response_product_query(user_input, classification):
    system_prompt = """You are a helpfull assistant. your task is to provide customer service support to customers. 
      
                You should follow the steps mention below to provide your reponse to the user.

                step 1: First identify if the query is about a specific product or products. based on the below query catagory:{query_catagory}

                step 2: If the user query is about a specific product or products. Then check if the requested product is present in the below products list : {context}

                step 3: Check if the user is making any assumptions regarding a product or products and if their assumptions are wrong 
                politelyy correct their assumptions.

                step 4: finally provide the user your response to thier query in a polite manner.
                you should respond by specifying each step in the following format :
                Use the following format:
                Step 1:{delimiter} <step 1 reasoning>
                Step 2:{delimiter} <step 2 reasoning>
                Step 3:{delimiter} <step 3 reasoning>
                Step 4:{delimiter} <step 4 reasoning>
                Response to user:{delimiter} <response to customer>

                Make sure to include {delimiter} to separate every step.
                """
    prompt = ChatPromptTemplate.from_messages(
        [("system", f"{system_prompt}"),
         ("user", "{input}")])

    vector_store = create_db(loaded_docs)

    retriver = vector_store.as_retriever()
    knowledge = retriver.invoke(user_input)

    retrival_cahin = {"context": itemgetter("context"), "input": itemgetter(
        "input"), "system_prompt": itemgetter("system_prompt"), "query_catagory": itemgetter("query_catagory"), "delimiter": itemgetter("delimiter")}

    finalChain = retrival_cahin | prompt | LLM | string_parser

    response = finalChain.invoke(
        {"input": user_input, "system_prompt": system_prompt, "query_catagory": classification, "context": knowledge, "delimiter": "###"})

    newresponse = response.split("###")
    moderated_response = openai.moderations.create(input=newresponse[-1])
    flag = moderated_response.results[0].flagged
    if not flag:
        return validate_response(newresponse, user_input, knowledge)
    else:
        return flag


def validate_response(system_response, user_input, knowledge):
    validation_system = """Your task is to evaluate the response generated by a customer service assistant to the user query.
                           You should check if the question is answered correctly, aslo if the question is answered based on the provided context
                           assistant response: {assistant_response}
                           question:{original_question}
                           context:{context}

                           You should respond with only one character Y or N:
                           where Y means the response is correcty addressing the question and answer is also based on the context.
                                  N means the assistant didn't give the desired output.
       """

    validation_prompt = ChatPromptTemplate.from_messages(
        [("system", f"{validation_system}")])

    validation_chain = validation_prompt | LLM | string_parser

    response = validation_chain.invoke(
        {"assistant_response": system_response, "original_question": user_input, "context": knowledge})

    if response == "Y":
        print(system_response[-1])
    else:
        print("The model did't answer the question succcessfully ")
